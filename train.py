from datasets import load_dataset  # Hugging Face Datasets ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë¶ˆëŸ¬ì˜´
from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer
import evaluate
from transformers import DataCollatorWithPadding, logging
import torch

# ë°ì´í„°ì…‹ ë¶ˆëŸ¬ì˜¤ê¸°
dataset = load_dataset('e9t/nsmc', trust_remote_code=True)

# í† í¬ë‚˜ì´ì €
pretrained_model = 'klue/bert-base'
tokenizer = AutoTokenizer.from_pretrained(pretrained_model)

def tokenize_function(example) : # ë°ì´í„°ì…‹ì˜ ë‚´ìš©ì— í•´ë‹¹í•˜ëŠ” 'document'ì— ëŒ€í•´ì„œ í† í°í™”í•˜ë„ë¡ í•¨ìˆ˜ ì„¤ì •
    return tokenizer(example['document'], truncation  = True, padding = True) # padding : ê¸¸ì´ì— ë§ê²Œ íŒ¨ë”©

tokenized_dataset = dataset.map(tokenize_function, batched = True)

# ë””ë°”ì´ìŠ¤ ì„¤ì • ë° gpuì‚¬ìš©ì—¬ë¶€ í™•ì¸
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"âœ… í˜„ì¬ ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}")
if device.type == 'cuda':
    print(f"ğŸ¯ GPU ì´ë¦„: {torch.cuda.get_device_name(0)}")
else:
    print("âš ï¸ í˜„ì¬ GPUë¥¼ ì‚¬ìš©í•˜ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.")

# ë¡œê·¸ ë ˆë²¨ ì„¤ì •
logging.set_verbosity_info()

# ëª¨ë¸ êµ¬ì¶•
model = AutoModelForSequenceClassification.from_pretrained(pretrained_model, num_labels = 2)
model.to(device) # ëª¨ë¸ì„ gpuë¡œ ì´ë™

# í›ˆë ¨ ì„¤ì •
args = TrainingArguments(
    output_dir='./results',
    learning_rate=2e-5,
    num_train_epochs=3,
    fp16=True, # gpu ë°˜ì •ë°€ë„ í›ˆë ¨ : ëª¨ë¸ ì¼ë¶€ ì—°ì‚°ì„ fp16(í‘œí˜„ë ¥ ë‚®ìŒ, ì—°ì‚°ë¹ ë¦„, ë©”ëª¨ë¦¬ ì ê²Œ)ìœ¼ë¡œ ìˆ˜í–‰í•˜ê³ , ì¼ë¶€ëŠ” fp32(í‘œí˜„ë ¥ ë†’ìŒ, ì—°ì‚° ëŠë¦¼, ë©”ëª¨ë¦¬ ë§ì´)ë¡œ ìœ ì§€í•´ì„œ ì†ë„ì™€ ë©”ëª¨ë¦¬ íš¨ìœ¨ì„ ë™ì‹œì— ì–»ëŠ” ë°©ì‹
    logging_strategy="steps", # 'epoch', 'no' / HuggingFace Traniner í›ˆë ¨ ì¤‘ ë¡œê·¸ë¥¼ ì–¸ì œ ì¶œë ¥í• ì§€ ì œì–´í•˜ëŠ” ì„¤ì •
    logging_steps=100,
    per_device_train_batch_size=32,
    weight_decay=0.01
)

# í‰ê°€í•¨ìˆ˜ ì„¤ì •
accuracy = evaluate.load('accuracy')

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    predictions = logits.argmax(axis = -1)
    return accuracy.compute(predictions=predictions, references = labels)

# í›ˆë ¨
data_collator = DataCollatorWithPadding(tokenizer=tokenizer) # ë°ì´í„° batchë¥¼ ë§Œë“¤ë©´ì„œ íŒ¨ë”© ê¸¸ì´ë¥¼ ìë™ìœ¼ë¡œ ëª» ë§ì¶°ì„œ ìƒê¸°ëŠ” ì˜¤ë¥˜ë¥¼ ë°©ì§€

trainer = Trainer( # model.train() í›ˆë ¨ëª¨ë“œë¥¼ ìë™ìœ¼ë¡œ í˜¸ì¶œ
    model = model,
    args = args,
    train_dataset=tokenized_dataset['train'],
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    tokenizer = tokenizer
)

trainer.train()

# ëª¨ë¸ ì €ì¥(inference.pyì—ì„œ ì¶”ë¡ í•¨ìˆ˜ê°€ í˜¸ì¶œí•œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡)
trainer.save_model('./model')
tokenizer.save_pretrained('./model')